name: NERV CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
    paths-ignore:
      - 'docs/**'
      - '*.md'
      - '.gitignore'
  pull_request:
    branches: [ main ]
    paths-ignore:
      - 'docs/**'
      - '*.md'
      - '.gitignore'
  workflow_dispatch:

env:
  CARGO_TERM_COLOR: always
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'
  RUST_VERSION: 'stable'

jobs:
  # Job 1: Rust Core Testing
  rust-tests:
    name: Rust Core Tests
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: ./backend/rust-core
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable
      with:
        toolchain: ${{ env.RUST_VERSION }}
        components: rustfmt, clippy

    - name: Cache Rust dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cargo/bin/
          ~/.cargo/registry/index/
          ~/.cargo/registry/cache/
          ~/.cargo/git/db/
          backend/rust-core/target/
        key: ${{ runner.os }}-cargo-${{ hashFiles('backend/rust-core/Cargo.lock') }}
        restore-keys: |
          ${{ runner.os }}-cargo-

    - name: Check Rust formatting
      run: cargo fmt -- --check

    - name: Run Clippy lints
      run: cargo clippy --all-targets --all-features -- -D warnings

    - name: Run Rust tests
      run: cargo test --verbose

    - name: Run Rust benchmarks
      run: cargo bench --verbose

    - name: Build release binary
      run: cargo build --release

    - name: Upload Rust artifacts
      uses: actions/upload-artifact@v3
      with:
        name: rust-binaries
        path: backend/rust-core/target/release/
        retention-days: 7

  # Job 2: Python API Testing
  python-tests:
    name: Python API Tests
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: ./backend/python-api

    services:
      # Neo4j test database
      neo4j:
        image: neo4j:5.15-community
        env:
          NEO4J_AUTH: neo4j/test-password
          NEO4J_PLUGINS: '["apoc"]'
        ports:
          - 7687:7687
          - 7474:7474
        options: >-
          --health-cmd "neo4j status"
          --health-interval 30s
          --health-timeout 10s
          --health-retries 5

      # Redis test cache
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 3

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: backend/python-api/requirements*.txt

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Check Python formatting with Black
      run: black --check --diff .

    - name: Check import sorting with isort
      run: isort --check-only --diff .

    - name: Run Flake8 linting
      run: flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics

    - name: Run MyPy type checking
      run: mypy app/ --ignore-missing-imports

    - name: Run Python tests with pytest
      env:
        NEO4J_URI: bolt://localhost:7687
        NEO4J_USER: neo4j
        NEO4J_PASSWORD: test-password
        REDIS_URL: redis://localhost:6379/0
        ENVIRONMENT: testing
      run: |
        pytest app/tests/ -v \
          --cov=app \
          --cov-report=xml \
          --cov-report=html \
          --cov-report=term-missing \
          --junitxml=test-results.xml

    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: python-test-results
        path: |
          backend/python-api/test-results.xml
          backend/python-api/htmlcov/
        retention-days: 30

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: backend/python-api/coverage.xml
        flags: python-api
        name: python-api-coverage

  # Job 3: Frontend Testing
  frontend-tests:
    name: Frontend Tests
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: ./frontend

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Node.js ${{ env.NODE_VERSION }}
      uses: actions/setup-node@v3
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json

    - name: Install Node.js dependencies
      run: npm ci

    - name: Run ESLint
      run: npm run lint

    - name: Run Prettier format check
      run: npm run format:check

    - name: Run TypeScript type checking
      run: npm run type-check

    - name: Run frontend tests
      run: npm run test:coverage

    - name: Build frontend
      run: npm run build

    - name: Upload frontend artifacts
      uses: actions/upload-artifact@v3
      with:
        name: frontend-build
        path: frontend/dist/
        retention-days: 7

  # Job 4: Integration Tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [rust-tests, python-tests]
    
    services:
      neo4j:
        image: neo4j:5.15-community
        env:
          NEO4J_AUTH: neo4j/test-password
          NEO4J_PLUGINS: '["apoc", "gds"]'
        ports:
          - 7687:7687
          - 7474:7474
        options: >-
          --health-cmd "neo4j status"
          --health-interval 30s
          --health-timeout 10s
          --health-retries 5

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 3

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download Rust binaries
      uses: actions/download-artifact@v3
      with:
        name: rust-binaries
        path: backend/rust-core/target/release/

    - name: Make Rust binaries executable
      run: chmod +x backend/rust-core/target/release/*

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Python dependencies
      working-directory: ./backend/python-api
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Run integration tests
      env:
        NEO4J_URI: bolt://localhost:7687
        NEO4J_USER: neo4j
        NEO4J_PASSWORD: test-password
        REDIS_URL: redis://localhost:6379/0
        RUST_LIB_PATH: ../../rust-core/target/release/nerv-geometry
        ENVIRONMENT: testing
      working-directory: ./backend/python-api
      run: |
        pytest tests/integration/ -v \
          --cov=app \
          --cov-append \
          --cov-report=xml \
          --junitxml=integration-test-results.xml

    - name: Upload integration test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results
        path: backend/python-api/integration-test-results.xml
        retention-days: 30

  # Job 5: Security Scanning
  security-scan:
    name: Security Scanning
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Run Rust security audit
      working-directory: ./backend/rust-core
      run: |
        cargo install cargo-audit
        cargo audit

    - name: Run Python security scan with Safety
      working-directory: ./backend/python-api
      run: |
        pip install safety
        safety check -r requirements.txt

    - name: Run Semgrep security analysis
      uses: returntocorp/semgrep-action@v1
      with:
        config: auto

    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'

    - name: Upload Trivy results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'

  # Job 6: Docker Build and Push
  docker-build:
    name: Docker Build and Push
    runs-on: ubuntu-latest
    needs: [rust-tests, python-tests, frontend-tests]
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop')

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2

    - name: Log in to Docker Hub
      uses: docker/login-action@v2
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v4
      with:
        images: nerv/geometry-engine
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}

    - name: Build and push Python API image
      uses: docker/build-push-action@v4
      with:
        context: ./backend/python-api
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

    - name: Build and push Frontend image
      uses: docker/build-push-action@v4
      with:
        context: ./frontend
        push: true
        tags: nerv/geometry-frontend:${{ steps.meta.outputs.version }}
        labels: ${{ steps.meta.outputs.labels }}

  # Job 7: Deploy to Staging
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [integration-tests, docker-build]
    if: github.ref == 'refs/heads/develop'
    environment: staging

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Deploy to staging environment
      env:
        STAGING_HOST: ${{ secrets.STAGING_HOST }}
        STAGING_USER: ${{ secrets.STAGING_USER }}
        STAGING_KEY: ${{ secrets.STAGING_SSH_KEY }}
      run: |
        echo "Deploying to staging environment"
        echo "This would typically involve:"
        echo "1. SSH to staging server"
        echo "2. Pull latest Docker images"
        echo "3. Update docker-compose configuration"
        echo "4. Restart services"
        echo "5. Run smoke tests"

  # Job 8: Deploy to Production
  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [integration-tests, docker-build, security-scan]
    if: github.ref == 'refs/heads/main'
    environment: production

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Deploy to production environment
      env:
        PRODUCTION_HOST: ${{ secrets.PRODUCTION_HOST }}
        PRODUCTION_USER: ${{ secrets.PRODUCTION_USER }}
        PRODUCTION_KEY: ${{ secrets.PRODUCTION_SSH_KEY }}
      run: |
        echo "Deploying to production environment"
        echo "This would typically involve:"
        echo "1. Blue-green deployment strategy"
        echo "2. Database migrations"
        echo "3. Health checks"
        echo "4. Rollback capability"
        echo "5. Monitoring alerts"

  # Job 9: Performance Testing
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: [integration-tests]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install performance testing tools
      run: |
        pip install locust pytest-benchmark

    - name: Run API performance tests
      working-directory: ./backend/python-api
      run: |
        echo "Running performance tests with Locust"
        echo "This would run load tests against the API endpoints"
        # locust -f tests/performance/locustfile.py --headless -u 10 -r 2 -t 30s

    - name: Run Rust benchmarks
      working-directory: ./backend/rust-core
      run: |
        cargo bench -- --output-format json | tee benchmark-results.json

    - name: Upload performance results
      uses: actions/upload-artifact@v3
      with:
        name: performance-results
        path: |
          backend/rust-core/benchmark-results.json
        retention-days: 30

  # Job 10: Notification
  notify:
    name: Notify Results
    runs-on: ubuntu-latest
    needs: [rust-tests, python-tests, frontend-tests, integration-tests]
    if: always()

    steps:
    - name: Notify on success
      if: ${{ needs.rust-tests.result == 'success' && needs.python-tests.result == 'success' && needs.frontend-tests.result == 'success' && needs.integration-tests.result == 'success' }}
      run: |
        echo "✅ All tests passed! NERV system is ready for deployment."

    - name: Notify on failure
      if: ${{ needs.rust-tests.result == 'failure' || needs.python-tests.result == 'failure' || needs.frontend-tests.result == 'failure' || needs.integration-tests.result == 'failure' }}
      run: |
        echo "❌ Some tests failed. Check the workflow results for details."
        exit 1

    - name: Create deployment summary
      run: |
        echo "## NERV CI/CD Pipeline Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Component | Status | Duration |" >> $GITHUB_STEP_SUMMARY
        echo "|-----------|--------|----------|" >> $GITHUB_STEP_SUMMARY
        echo "| Rust Core | ${{ needs.rust-tests.result }} | - |" >> $GITHUB_STEP_SUMMARY
        echo "| Python API | ${{ needs.python-tests.result }} | - |" >> $GITHUB_STEP_SUMMARY
        echo "| Frontend | ${{ needs.frontend-tests.result }} | - |" >> $GITHUB_STEP_SUMMARY
        echo "| Integration | ${{ needs.integration-tests.result }} | - |" >> $GITHUB_STEP_SUMMARY